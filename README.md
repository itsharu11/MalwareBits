# Malware-Prediction
A Malware Prediction model that predicted if the PE format file is malicious or legitimate. 

Dependencies
============

* pandas ```pip install pandas```
* numpy ```pip install numpy```
* pickle ```pip install pickle``` or ```pip install pickle-mixin```
* scipy ```pip install scipy```
* scikit ```pip install -U scikit-learn```
* pefile ```pip install pefile```
* seaborn ```pip install seaborn```

To run the program
===================
* ```python learn.py```
* ```python check.py PEfilename```

# learn.py

```python
import pandas as pd
import numpy as np
```
These are standard libraries for data manipulation and numerical operations in Python. Pandas provides data structures and functions for efficiently working with structured data, while NumPy offers support for mathematical operations on multi-dimensional arrays.

```python
import pickle
```
The `pickle` module is used for serializing and deserializing Python objects. It allows you to save and load Python objects to and from disk.

```python
import seaborn as sn
import matplotlib.pyplot as plt
```
These libraries are used for data visualization. Seaborn provides a high-level interface for creating informative and attractive statistical graphics, while Matplotlib is a powerful plotting library in Python.

```python
import sklearn.ensemble as ske
from sklearn import tree, linear_model, neighbors
```
These imports are related to machine learning algorithms and models. `sklearn.ensemble` provides ensemble methods such as random forests and gradient boosting. `tree`, `linear_model`, and `neighbors` are modules from scikit-learn that offer decision tree, linear regression, and k-nearest neighbors algorithms, respectively.

```python
from sklearn.model_selection import cross_validate, train_test_split
from sklearn.feature_selection import SelectFromModel
```
These imports are for model evaluation and feature selection. `cross_validate` is used for cross-validation of models, while `train_test_split` is used for splitting the data into training and testing sets. `SelectFromModel` allows for selecting important features based on a specified model.

```python
import joblib
```
The `joblib` library is used for efficiently saving and loading Python objects, particularly scikit-learn models.

```python
from sklearn.naive_bayes import GaussianNB
```
This import is specifically for the Gaussian Naive Bayes algorithm, a probabilistic classification algorithm commonly used in machine learning.

```python
from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report
```

These imports provide various metrics and functions for evaluating classification models. `confusion_matrix` computes the confusion matrix, while `recall_score`, `precision_score`, and `f1_score` calculate different evaluation metrics based on the confusion matrix. `classification_report` generates a text report showing the main classification metrics.

```python
import json
```
The `json` module is used for working with JSON (JavaScript Object Notation) data in Python. It provides functions for encoding and decoding JSON data.

These imports indicate that the code likely involves data analysis, machine learning, model evaluation, and visualization tasks.

#code

``` python
import pandas as pd

data = pd.read_csv('data.csv', sep='|')
X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values
y = data['legitimate'].values
```

The pandas library is imported. The 'data.csv' file is loaded using `pd.read_csv()`, with the 'sep' parameter set to '|' to specify the delimiter used in the CSV file. The 'Name', 'md5', and 'legitimate' columns are dropped from the DataFrame using `data.drop()`, and the remaining columns are assigned to the variable X as an array of values. The 'legitimate' column is extracted from the DataFrame and assigned to the variable y as an array of values.


```python
print('Researching important feature based on %i total features\n' % X.shape[1])

# Feature selection using Trees Classifier (ExtraTree)
fsel = ske.ExtraTreesClassifier().fit(X, y)
model = SelectFromModel(fsel, prefit=True)
X_new = model.transform(X)
nb_features = X_new.shape[1]
```

The number of total features in variable X is printed using the `print()` function. The feature selection process is then performed using the Trees Classifier (ExtraTree) from the `sklearn.ensemble` module. The `fit()` function is called on the ExtraTreesClassifier with X and y as input. 

A `SelectFromModel` object is created with the fitted ExtraTreesClassifier (`fsel`) as the model. The `prefit=True` argument indicates that the model has already been trained. The `transform()` function is then used to select the important features based on the model, and the transformed feature matrix is assigned to the variable X_new. 

Finally, the number of selected features is determined by accessing the shape of X_new and retrieving the number of columns, which is assigned to the variable nb_features.

```python
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.5, shuffle=True, random_state=100)

features = []

print('%i features identified as important: \n' % nb_features)
```

The feature matrix `X_new` and the target variable `y` are split into training and testing sets using the `train_test_split()` function from the `sklearn.model_selection` module. The `test_size` parameter is set to 0.5, indicating that the testing set will contain 50% of the data. The `shuffle` parameter is set to `True` to randomly shuffle the data before splitting, and the `random_state` parameter is set to 100 for reproducibility.

An empty list `features` is initialized.

The number of important features (`nb_features`) is printed using the `print()` function.

```python
indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))

# XXX : take care of the feature order
for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):
    features.append(data.columns[2+f])
```

The indices of the important features are sorted based on their feature importances using `np.argsort()`. The `[::-1]` notation is used to sort the indices in descending order. The `[:nb_features]` part is used to select the top `nb_features` indices.

A loop is then used to iterate over the range of `nb_features`. Within the loop, the feature index, feature name, and feature importance score are printed using the `print()` function. The feature index is calculated as `f + 1`, and the feature name is retrieved from the `data.columns` array using the corresponding index from `2+indices[f]`. The feature importance score is obtained from `fsel.feature_importances_` using the corresponding index from `indices[f]`.

Another loop is used to iterate over the sorted indices of the important features. Within this loop, the feature names are appended to the `features` list using the corresponding index from `2+f`.


```python
# Algorithm comparison
algorithms = {
        "Regression": linear_model.LinearRegression(),
        "DecisionTree": tree.DecisionTreeClassifier(max_depth=10),
        "RandomForest": ske.RandomForestClassifier(n_estimators=50),
        "GradientBoosting": ske.GradientBoostingClassifier(n_estimators=50),
        "AdaBoost": ske.AdaBoostClassifier(n_estimators=100),
        "GNB": GaussianNB(),
        "k-NN": neighbors.KNeighborsRegressor()
    }

results = {}
print("\nNow testing algorithms")
for algo in algorithms:
    clf = algorithms[algo]
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    precentage = score * 100
    print("%s : %f %%" % (algo, precentage))
    results[algo] = score

with open('barchart.json', 'w') as json_file:
    json.dump(results, json_file)

winner = max(results, key=results.get)
print('\nWinner algorithm is %s with a %f %% success' % (winner, results[winner]*100))
```

A comparison of different algorithms is performed. The algorithms are defined in the `algorithms` dictionary, where the keys represent the algorithm names and the values represent the corresponding algorithm objects.

A loop is then used to iterate over each algorithm. Within the loop, the algorithm object is accessed using `algorithms[algo]`, and it is trained on the training data using the `fit()` method. The accuracy score is calculated using the `score()` method with the test data, and it is converted to a percentage by multiplying it with 100. The algorithm name and accuracy percentage are printed using the `print()` function, and the accuracy score is stored in the `results` dictionary with the algorithm name as the key.

The results are saved in a JSON file named 'barchart.json' using the `json.dump()` function.

Finally, the algorithm with the highest accuracy score is determined by finding the maximum value in the `results` dictionary using `max()` and retrieving the corresponding key. The winner algorithm and its success percentage are printed using the `print()` function.


```python
# Save the algorithm and the feature list for later predictions
print('Saving algorithm and feature list in classifier directory...')
joblib.dump(algorithms[winner], 'classifier/classifier.pkl')
open('classifier/features.pkl', 'wb').write(pickle.dumps(features))
print('Saved')

# Evaluation
print('Evaluation... \n')
clf = algorithms[winner]
res = clf.predict(X_test)
```

The winning algorithm is selected based on the maximum accuracy score obtained from the previous comparison. The algorithm object corresponding to the winner is accessed using `algorithms[winner]`.

The winning algorithm, along with the feature list, is saved for later predictions. The algorithm is saved using the `joblib.dump()` function and stored in the 'classifier/classifier.pkl' file. The feature list is serialized using the `pickle.dumps()` function and saved in the 'classifier/features.pkl' file.

After saving the algorithm and feature list, the evaluation phase begins. The selected algorithm is assigned to the variable `clf`. The `predict()` method is used to make predictions on the test data (`X_test`), and the results are stored in the `res` variable.



```python
# outcome values order in sklearn
tp, fn, fp, tn = confusion_matrix(y_test, res, labels=[1, 0]).reshape(-1)
print('tp fn fp tn : \n', tp, fn, fp, tn)
```

The confusion matrix is computed using the `confusion_matrix()` function from scikit-learn. It takes the true labels `y_test` and the predicted labels `res` as input. The `labels` parameter is used to specify the order of the outcome values, where 1 represents the positive class and 0 represents the negative class.

The confusion matrix is reshaped using `.reshape(-1)` to obtain the true positive (tp), false negative (fn), false positive (fp), and true negative (tn) values. These values represent the counts of each outcome in the confusion matrix.

Finally, the values of tp, fn, fp, and tn are printed using the `print()` function.


```python
# classification report for precision, recall, f1-score, and accuracy
matrix = classification_report(y_test, res, labels=[1, 0])
print('Classification report:\n', matrix)
```

The `classification_report()` function from scikit-learn is used to generate a classification report. It takes the true labels `y_test` and the predicted labels `res` as input. The `labels` parameter is used to specify the order of the outcome values, where 1 represents the positive class and 0 represents the negative class.

The classification report provides metrics such as precision, recall, f1-score, and accuracy for each class. It summarizes the model's performance on the test set.

The classification report is stored in the variable `matrix`, and it is printed using the `print()` function.


```python
print('Classification report:\n', matrix)
precision_positive = precision_score(y_test, res, pos_label=1)
precision_negative = precision_score(y_test, res, pos_label=0)
print('True Positive: ', precision_positive * 100)
print('True Negative: ', precision_negative * 100)
```

The classification report is printed using the `print()` function as specified. Then, the precision score is calculated separately for the positive class (True Positive) and the negative class (True Negative) using the `precision_score()` function from scikit-learn. The `pos_label` parameter is used to specify the positive class label.

The precision scores for the positive and negative classes are multiplied by 100 to convert them to percentages and then printed using the `print()` function.


```python
recall_sensitivity = recall_score(y_test, res, pos_label=1)
recall_specificity = recall_score(y_test, res, pos_label=0)
print('Sensitivity: ', recall_sensitivity * 100)
print('Specificity: ', recall_specificity * 100)
```

In the code snippet above, the recall score is calculated separately for the positive class (sensitivity) and the negative class (specificity) using the `recall_score()` function from scikit-learn. The `pos_label` parameter is used to specify the positive class label.

The sensitivity and specificity scores are multiplied by 100 to convert them to percentages and then printed using the `print()` function.




```python

# Confusion matrix
confusion_matrix = pd.crosstab(y_test, res, rownames=['Actual'], colnames=['Predicted'], margins=True)

# Plotting the confusion matrix as a heatmap
sn.heatmap(confusion_matrix, annot=True, fmt='d')
plt.savefig('ConfusionMatrix.png', dpi=300, bbox_inches='tight')
plt.show()
```

In the code snippet above, we use the `pd.crosstab()` function from pandas to create a confusion matrix based on the actual values (`y_test`) and predicted values (`res`). We set the row names as "Actual" and column names as "Predicted". The `margins=True` parameter adds the row and column totals to the matrix.

Then, we use `seaborn` to create a heatmap visualization of the confusion matrix. The `annot=True` parameter adds the numerical values to the heatmap cells, and `fmt='d'` formats the values as integers.

Finally, we save the heatmap as an image file named "ConfusionMatrix.png" using `plt.savefig()`, and display the heatmap using `plt.show()`.


# check.py

The provided code appears to be a Python script for detecting whether a file is malicious or legitimate. Here's a breakdown of its functionality:

1. The script imports various modules such as `pefile`, `array`, `math`, `pickle`, `joblib`, `argparse`, `os`, `sys`, `shutil`, `time`, `re`, and `pandas`.

2. It defines a function called `cutit` that takes a string `s` and an integer `n` and returns a substring of `s` starting from index `n`.

3. The script creates a Flask application instance called `app`.

4. It defines a route for the root URL ("/") that renders a template called "predict.html".

5. It defines another route ("/uploader") that handles file uploads. If the request method is POST, it retrieves the uploaded file, saves it with a secure filename, and then proceeds with the following steps:

   a. Loads a pre-trained classifier from the file "classifier/classifier.pkl".

   b. Loads a list of features used for classification from the file "classifier/features.pkl".

   c. Extracts information from the uploaded file using the `extract_infos` function.

   d. Extracts the specific features required for classification from the extracted information.

   e. Predicts whether the file is malicious or legitimate using the loaded classifier.

   f. Renders a template called "result.html" with the predicted result.

6. The script defines a function called `get_entropy` that calculates the entropy of a given data sequence.

7. It defines a function called `get_resources` that extracts resources (such as entropy and size) from a given PE file.

8. It defines a function called `get_version_info` that retrieves version information from a PE file.

9. It defines a function called `extract_infos` that takes a file path as input and returns a dictionary containing various extracted information from the PE file, including file headers, section information, import details, export details, resource details, and version information.

10. The script includes a conditional block that executes when the script is run directly (not imported as a module). Inside this block:

    a. It parses the command-line arguments to retrieve the file path to be tested.

    b. Loads the classifier and features.

    c. Calls the `extract_infos` function to extract information from the specified file.

    d. Retrieves the specific features required for classification.

    e. Predicts whether the file is malicious or legitimate using the loaded classifier.

    f. Prints the result indicating whether the file is malicious or legitimate.

11. The script includes another conditional block that executes when the script is run directly (not imported as a module). Inside this block, it starts the Flask application in debug mode.

This code seems to implement a basic file upload functionality using Flask and applies a pre-trained classifier to determine if the uploaded file is malicious or legitimate.